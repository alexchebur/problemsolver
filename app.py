import streamlit as st
import google.generativeai as genai
import time
import requests
import traceback
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import random
import base64
import os
from datetime import datetime
from bs4 import BeautifulSoup
from websearch import WebSearcher
from typing import List, Tuple
import re
from report import create_html_report
from prompts import (
    PROMPT_FORMULATE_PROBLEM_AND_QUERIES,
    PROMPT_APPLY_COGNITIVE_METHOD,
    PROMPT_GENERATE_REFINEMENT_QUERIES,
    PROMPT_GENERATE_FINAL_CONCLUSIONS
)
from converters import (
    convert_uploaded_file_to_markdown, 
    convert_excel_to_markdown_for_analysis  # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API
api_key = st.secrets['GEMINI_API_KEY']
if not api_key:
    st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ API-–∫–ª—é—á")
    st.stop()

genai.configure(
    api_key=api_key,
    transport='rest',
    client_options={
        'api_endpoint': 'generativelanguage.googleapis.com/'
    }
)

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
searcher = WebSearcher()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
if 'current_doc_text' not in st.session_state:
    st.session_state.current_doc_text = ""
if 'processing' not in st.session_state:
    st.session_state.processing = False
if 'report_content' not in st.session_state:
    st.session_state.report_content = None
if 'problem_formulation' not in st.session_state:
    st.session_state.problem_formulation = ""
if 'generated_queries' not in st.session_state:
    st.session_state.generated_queries = []
if 'search_results' not in st.session_state:
    st.session_state.search_results = ""
if 'internal_dialog' not in st.session_state:
    st.session_state.internal_dialog = ""
# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
if 'time_series_analysis' not in st.session_state:
    st.session_state.time_series_analysis = None

# –ú–æ–¥–µ–ª—å
model = genai.GenerativeModel('gemini-2.5-flash-lite-preview-06-17')

# –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥–∏–∫–∏
CORE_METHODS = [
    "First Principles Thinking",
    "Pareto Principle (80/20 Rule)",
    "Occam's Razor",
    "System 1 vs System 2 Thinking",
    "Second-Order Thinking"
]

ADDITIONAL_METHODS = [
    "Inversion (thinking backwards)",
    "Opportunity Cost",
    "Margin of Diminishing Returns",
    "Hanlon's Razor",
    "Confirmation Bias",
    "Availability Heuristic",
    "Parkinson's Law",
    "Loss Aversion",
    "Switching Costs",
    "Circle of Competence",
    "Regret Minimization",
    "Leverage Points",
    "Lindy Effect",
    "Game Theory",
    "Antifragility",
    "–¢–µ–æ—Ä–∏–∏ –†–µ—à–µ–Ω–∏—è –ò–∑–æ–±—Ä–µ—Ç–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–¥–∞—á"
]

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤
CONTEXT_CONFIG = {
    'problem_formulation': {
        'doc_text': True,
        'original_query': True,
        'search_results': False,
        'method_results': False,
        'time_series': False  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
    },
    'cognitive_method': {
        'doc_text': True,
        'original_query': True,
        'search_results': True,
        'method_results': False,
        'time_series': True   # –ù–æ–≤–æ–µ –ø–æ–ª–µ
    },
    'refinement_queries': {
        'doc_text': False,
        'original_query': True,
        'search_results': True,
        'method_results': True,
        'time_series': True   # –ù–æ–≤–æ–µ –ø–æ–ª–µ
    },
    'final_conclusions': {
        'doc_text': False,
        'original_query': True,
        'search_results': True,
        'method_results': True,
        'time_series': True   # –ù–æ–≤–æ–µ –ø–æ–ª–µ
    }
}

def get_current_date():
    return datetime.now().strftime("%Y-%m-%d")

def build_context(context_type: str) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
    config = CONTEXT_CONFIG[context_type]
    context_parts = []
    
    if config['doc_text'] and st.session_state.current_doc_text:
        context_parts.append(f"–î–æ–∫—É–º–µ–Ω—Ç: {st.session_state.current_doc_text[:100000]}")
    
    if config['original_query'] and 'input_query' in st.session_state:
        context_parts.append(f"–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {st.session_state.input_query}")
    
    if config['search_results'] and st.session_state.search_results:
        context_parts.append(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: {st.session_state.search_results[:20000]}")
    
    if config['method_results'] and hasattr(st.session_state, 'method_results'):
        method_results = "\n\n".join(
            [f"{method}:\n{result}" for method, result in st.session_state.method_results.items()]
        )
        context_parts.append(f"–ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–∏–∫: {method_results[:10000]}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    if config['time_series'] and st.session_state.time_series_analysis:
        context_parts.append(f"–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤: {st.session_state.time_series_analysis}")
    
    return "\n\n".join(context_parts)

# –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
def analyze_time_series(file_content: bytes) -> str:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏–∑ Excel —Ñ–∞–π–ª–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç"""
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Excel –≤ Markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        markdown_data = convert_excel_to_markdown_for_analysis(file_content)
        
        analytic_prompt = f"""
**–ó–∞–¥–∞—á–∞:** –ü—Ä–æ–≤–µ–¥–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –ø–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ö–µ–º–µ: 
1. –î–µ—Å–∫—Ä–∏–ø—Ç–∏–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (—Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ)
2. –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –∏ –≤—ã–±—Ä–æ—Å–æ–≤
3. –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–≤–∫–ª—é—á–∞—è –∫—Ä–æ—Å—Å-–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –∑–∞–ø–∞–∑–¥—ã–≤–∞—é—â–∏–µ)
4. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ (–ª–∏–Ω–µ–π–Ω—ã–µ/–Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ, —Ç–æ—á–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è)
5. –°–µ–∑–æ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
6. –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 3-5 –ø–µ—Ä–∏–æ–¥–æ–≤ —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
7. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—Ä–æ–±–ª–µ–º—ã

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏—Å—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã
- –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

**–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–±–ª–µ–º—ã:**
{st.session_state.problem_formulation}

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:**
{markdown_data}
"""
        
        response = model.generate_content(
            analytic_prompt,
            generation_config={
                "temperature": st.session_state.temperature * 0.7,
                "max_output_tokens": 10000
            },
            request_options={'timeout': 240}
        )
        return response.text
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤: {str(e)}\n\n{traceback.format_exc()}"



def formulate_problem_and_queries():
    """–≠—Ç–∞–ø 1: –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    context = build_context('problem_formulation')
    
    prompt = PROMPT_FORMULATE_PROBLEM_AND_QUERIES.format(
        query=st.session_state.input_query.strip(),
        doc_text=st.session_state.current_doc_text[:200000] if st.session_state.current_doc_text else "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞"
    )
    
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": st.session_state.temperature,
                "max_output_tokens": 4000
            },
            request_options={'timeout': 120}
        )
        
        result = response.text
        
        problem = ""
        internal_dialog = ""
        queries = []
        
        if "–ü–†–û–ë–õ–ï–ú–ê:" in result:
            problem_part = result.split("–ü–†–û–ë–õ–ï–ú–ê:")[1]
            if "–†–ê–°–°–£–ñ–î–ï–ù–ò–Ø:" in problem_part:
                problem = problem_part.split("–†–ê–°–°–£–ñ–î–ï–ù–ò–Ø:")[0].strip()
                internal_dialog_part = problem_part.split("–†–ê–°–°–£–ñ–î–ï–ù–ò–Ø:")[1]
                if "–ó–ê–ü–†–û–°–´:" in internal_dialog_part:
                    internal_dialog = internal_dialog_part.split("–ó–ê–ü–†–û–°–´:")[0].strip()
                    queries_part = internal_dialog_part.split("–ó–ê–ü–†–û–°–´:")[1]
                    for line in queries_part.split('\n'):
                        if line.strip() and line.strip()[0].isdigit():
                            query_text = line.split('.', 1)[1].strip() if '. ' in line else line.strip()
                            queries.append(query_text)
        
        if not problem:
            problem = result
        if not queries:
            last_lines = result.split('\n')[-10:]
            for line in last_lines:
                if line.strip() and line.strip()[0].isdigit() and '.' in line:
                    query_text = line.split('.', 1)[1].strip()
                    queries.append(query_text)
            if len(queries) > 5:
                queries = queries[:5]
        
        st.session_state.problem_formulation = problem
        st.session_state.internal_dialog = internal_dialog
        st.session_state.generated_queries = queries[:5]
        
        return result, queries
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º—ã: {str(e)}")
        return f"–û—à–∏–±–∫–∞: {str(e)}", []

def apply_cognitive_method(method_name: str):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—É—é –º–µ—Ç–æ–¥–∏–∫—É –∫ –ø—Ä–æ–±–ª–µ–º–µ"""
    context = build_context('cognitive_method')
    
    prompt = PROMPT_APPLY_COGNITIVE_METHOD.format(
        method_name=method_name,
        problem_formulation=st.session_state.problem_formulation,
        context=context
    )
    
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": st.session_state.temperature,
                "max_output_tokens": 12000
            },
            request_options={'timeout': 180}
        )
        return response.text
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ {method_name}: {str(e)}"

def generate_refinement_queries() -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É—Ç–æ—á–Ω—è—é—â–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    context = build_context('refinement_queries')
    
    prompt = PROMPT_GENERATE_REFINEMENT_QUERIES.format(
        context=context[:100000]
    )
    
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": st.session_state.temperature * 0.5,
                "max_output_tokens": 2000
            },
            request_options={'timeout': 90}
        )
        result = response.text
        
        queries = []
        for line in result.split('\n'):
            if line.strip() and line.strip()[0].isdigit():
                query_text = line.split('.', 1)[1].strip() if '. ' in line else line.strip()
                queries.append(query_text)
        
        return queries[:5]
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {str(e)}")
        return []

def generate_final_conclusions() -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    context = build_context('final_conclusions')
    
    prompt = PROMPT_GENERATE_FINAL_CONCLUSIONS.format(
        problem_formulation=st.session_state.problem_formulation,
        analysis_context=context[:100000]
    )
    
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": st.session_state.temperature * 0.7,
                "max_output_tokens": 9000
            },
            request_options={'timeout': 120}
        )
        return response.text
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–æ–≤: {str(e)}"

def generate_response():
    st.session_state.processing = True
    st.session_state.report_content = None
    status_area = st.empty()
    progress_bar = st.progress(0)

    try:
        try:
            test_response = requests.get("https://www.google.com", timeout=10)
            st.info(f"–°–µ—Ç–µ–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: {'OK' if test_response.status_code == 200 else '–ü—Ä–æ–±–ª–µ–º—ã'}")
        except Exception as e:
            st.error(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

        query = st.session_state.input_query.strip()
        if not query:
            status_area.warning("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å")
            return

        status_area.info("üîç –§–æ—Ä–º—É–ª–∏—Ä—É—é –ø—Ä–æ–±–ª–µ–º—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã...")
        problem_result, queries = formulate_problem_and_queries()
        if not queries:
            queries = [query]
            st.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ (LLM –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—ã)")
        
        if st.session_state.internal_dialog:
            st.sidebar.subheader("üß† –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ò–ò")
            st.sidebar.text_area(
                "",
                value=st.session_state.internal_dialog,
                height=300,
                label_visibility="collapsed"
            )
        else:
            st.sidebar.warning("–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
        
        with st.expander("‚úÖ –≠—Ç–∞–ø 1: –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã", expanded=False):
            st.subheader("–°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞")
            st.write(st.session_state.problem_formulation)
            
            st.subheader("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
            st.write(queries)
            
            st.subheader("–ü–æ–ª–Ω—ã–π –≤—ã–≤–æ–¥ LLM")
            st.code(problem_result, language='text')
        
        full_report = f"### –≠—Ç–∞–ø 1: –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã ###\n\n{problem_result}\n\n"
        full_report += f"–°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞: {st.session_state.problem_formulation}\n\n"
        full_report += f"–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:\n{st.session_state.internal_dialog}\n\n"
        full_report += f"–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\n" + "\n".join([f"{i+1}. {q}" for i, q in enumerate(queries)]) + "\n\n"

        status_area.info("üîç –í—ã–ø–æ–ª–Ω—è—é –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...")
        all_search_results = ""
        
        for i, search_query in enumerate(queries):
            try:
                clean_query = search_query.replace('"', '').strip()
                search_result_list = searcher.perform_search(clean_query, max_results=3, full_text=True)
                
                formatted_results = []
                for j, r in enumerate(search_result_list, 1):
                    content = r.get('full_content', r.get('snippet', ''))
                    formatted_results.append(
                        f"–†–µ–∑—É–ª—å—Ç–∞—Ç {j}: {r.get('title', '')}\n"
                        f"–ö–æ–Ω—Ç–µ–Ω—Ç: {content[:5000]}{'...' if len(content) > 5000 else ''}\n"
                        f"URL: {r.get('url', '')}\n"
                    )

                search_result_str = "\n\n".join(formatted_results)
                all_search_results += f"### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_query}':\n\n{search_result_str}\n\n"
                
                st.sidebar.subheader(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{search_query}'")
                for j, r in enumerate(search_result_list, 1):
                    title = r.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                    url = r.get('url', '')
                    snippet = r.get('snippet', '')

                    if url and not url.startswith('http'):
                        url = 'https://' + url

                    st.sidebar.markdown(f"**{j}. {title}**")
                    if url:
                        st.sidebar.markdown(f"[{url}]({url})")
                    if snippet:
                        st.sidebar.caption(snippet[:300] + ('...' if len(snippet) > 300 else ''))
                    st.sidebar.write("---")
                
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{search_query}': {str(e)}")
                all_search_results += f"### –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{search_query}': {str(e)}\n\n"
        
        st.session_state.search_results = all_search_results
        
        status_area.info("‚öôÔ∏è –ü—Ä–∏–º–µ–Ω—è—é –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥–∏–∫–∏...")
        
        all_methods = CORE_METHODS.copy()
        if st.session_state.selected_methods:
            all_methods += [m for m in st.session_state.selected_methods if m not in CORE_METHODS]
        
        st.session_state.method_results = {}
        
        for i, method in enumerate(all_methods):
            progress = int((i + 1) / (len(all_methods) + 1) * 100)
            progress_bar.progress(progress)
            
            status_area.info(f"‚öôÔ∏è –ü—Ä–∏–º–µ–Ω—è—é {method}...")
            try:
                result = apply_cognitive_method(method)
                st.session_state.method_results[method] = result
                
                st.subheader(f"‚úÖ {method}")
                st.text_area("", value=result, height=300, label_visibility="collapsed")
                
                full_report += f"### –ú–µ—Ç–æ–¥–∏–∫–∞: {method} ###\n\n{result}\n\n"
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ {method}: {str(e)}")
                full_report += f"### –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ {method}: {str(e)}\n\n"
            
            time.sleep(1)
        
        status_area.info("üîç –í—ã–ø–æ–ª–Ω—è—é —É—Ç–æ—á–Ω—è—é—â–∏–π –ø–æ–∏—Å–∫...")
        refinement_search_results = ""
    
        refinement_queries = generate_refinement_queries()
    
        if refinement_queries:
            st.sidebar.subheader("üîé –£—Ç–æ—á–Ω—è—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã")
            for i, query in enumerate(refinement_queries):
                st.sidebar.write(f"{i+1}. {query}")
            
            for i, query in enumerate(refinement_queries):
                try:
                    clean_query = query.replace('"', '').strip()
                    search_results = searcher.perform_search(clean_query, max_results=2, full_text=True)
                
                    formatted = []
                    for j, r in enumerate(search_results, 1):
                        content = r.get('full_content', r.get('snippet', ''))
                        formatted.append(
                            f"–†–µ–∑—É–ª—å—Ç–∞—Ç {j}: {r.get('title', '')}\n"
                            f"–ö–æ–Ω—Ç–µ–Ω—Ç: {content[:3000]}{'...' if len(content) > 3000 else ''}\n"
                            f"URL: {r.get('url', '')}\n"
                        )
                
                    refinement_search_results += f"### –£—Ç–æ—á–Ω—è—é—â–∏–π –∑–∞–ø—Ä–æ—Å '{query}':\n\n"
                    refinement_search_results += "\n\n".join(formatted) + "\n\n"
                
                except Exception as e:
                    refinement_search_results += f"### –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {str(e)}\n\n"
    
        status_area.info("üìù –§–æ—Ä–º–∏—Ä—É—é –∏—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã...")
        progress_bar.progress(95)
        try:
            conclusions = generate_final_conclusions()
            
            st.subheader("üìù –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã")
            st.text_area("", value=conclusions, height=400, label_visibility="collapsed")
            
            full_report += f"### –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã ###\n\n{conclusions}\n\n"
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–æ–≤: {str(e)}")
            full_report += f"### –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–æ–≤: {str(e)}\n\n"
        
        st.session_state.report_content = full_report
        progress_bar.progress(100)
        status_area.success("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
    except Exception as e:
        st.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        traceback.print_exc()
    finally:
        st.session_state.processing = False

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
with st.sidebar:
    st.title("Troubleshooter")
    st.subheader("–†–µ—à–∞—Ç–µ–ª—å –ø—Ä–æ–±–ª–µ–º")
    
    st.markdown("### –í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã:")
    selected_methods = st.multiselect(
        "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥–∏–∫–∏:",
        ADDITIONAL_METHODS,
        key="selected_methods"
    )

# –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
st.title("Troubleshooter - –†–µ—à–∞—Ç–µ–ª—å –ø—Ä–æ–±–ª–µ–º")
st.subheader("–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤")

col1, col2 = st.columns([3, 1])
with col1:
    st.text_input(
        "–í–∞—à –∑–∞–ø—Ä–æ—Å:",
        placeholder="–û–ø–∏—à–∏—Ç–µ –≤–∞—à—É –ø—Ä–æ–±–ª–µ–º—É...",
        key="input_query"
    )
with col2:
    st.slider(
        "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞:",
        0.0, 1.0, 0.3, 0.1,
        key="temperature"
    )



# –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑—á–∏–∫ —Ñ–∞–π–ª–æ–≤
uploaded_file = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (DOCX, XLSX, PPTX, PDF):",
    type=["docx", "xlsx", "pptx", "pdf"],
    key="uploaded_file"
)

if uploaded_file:
    markdown_content = convert_uploaded_file_to_markdown(uploaded_file)
    if markdown_content is not None:
        st.session_state.current_doc_text = markdown_content
        st.success(f"üìÇ Document converted: {len(st.session_state.current_doc_text)} characters")
    else:
        st.error("üö® Unsupported file type or conversion error")
        st.session_state.current_doc_text = ""

if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ", disabled=st.session_state.processing):
    generate_response()


# –ù–æ–≤—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
st.markdown("---")
st.subheader("–ê–Ω–∞–ª–∏–∑ —Ä—è–¥–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º)")
time_series_file = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª XLSX —Å —Ä—è–¥–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–Ω–µ –∑–∞–±—É–¥—å—Ç–µ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ —Å—Ç—Ä–æ–∫–µ –∑–∞–ø—Ä–æ—Å–∞ –≤—ã—à–µ):",
    type=["xlsx"],
    key="time_series_file"
)

# –î–æ–±–∞–≤–ª—è–µ–º —Å—é–¥–∞ –±–ª–æ–∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
if time_series_file is not None:
    with st.expander("üîç –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫)", expanded=True):
        try:
            df = pd.read_excel(time_series_file)
            st.dataframe(df.head(10))
            st.write("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã:", df.dtypes)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            for col in df.columns:
                try:
                    df[col] = pd.to_numeric(df[col], errors='ignore')
                except:
                    pass
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            st.markdown("### üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
            
            # –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –æ—Å–∏ X –∏ Y
            col1, col2 = st.columns(2)
            with col1:
                x_col = st.selectbox(
                    "–°—Ç–æ–ª–±–µ—Ü –¥–ª—è –æ—Å–∏ X (–¥–∞—Ç–∞/–∫–∞—Ç–µ–≥–æ—Ä–∏—è):",
                    df.columns,
                    index=min(0, len(df.columns)-1)
                )
            with col2:
                y_col = st.selectbox(
                    "–°—Ç–æ–ª–±–µ—Ü –¥–ª—è –æ—Å–∏ Y (—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è):",
                    [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])],
                    index=min(1, len(df.columns)-1)
                )
            
            # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –≥—Ä–∞—Ñ–∏–∫–∞
            plot_type = st.radio(
                "–¢–∏–ø –≥—Ä–∞—Ñ–∏–∫–∞:",
                ["–õ–∏–Ω–µ–π–Ω—ã–π", "–°—Ç–æ–ª–±—á–∞—Ç—ã–π", "–¢–æ—á–µ—á–Ω—ã–π"],
                horizontal=True
            )
            
            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
            fig, ax = plt.subplots(figsize=(10, 5))
            
            if plot_type == "–õ–∏–Ω–µ–π–Ω—ã–π":
                ax.plot(df[x_col], df[y_col], marker='o')
            elif plot_type == "–°—Ç–æ–ª–±—á–∞—Ç—ã–π":
                ax.bar(df[x_col], df[y_col])
            else:
                ax.scatter(df[x_col], df[y_col])
            
            ax.set_title(f"{plot_type} –≥—Ä–∞—Ñ–∏–∫: {y_col} –ø–æ {x_col}")
            ax.set_xlabel(x_col)
            ax.set_ylabel(y_col)
            plt.xticks(rotation=45)
            st.pyplot(fig)
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")

if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ", key="analyze_ts_button"):
    if time_series_file is not None:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        st.session_state.time_series_raw = time_series_file.getvalue()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        with st.spinner("üî¢ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ä—è–¥—ã –¥–∞–Ω–Ω—ã—Ö..."):
            analysis_result = analyze_time_series(st.session_state.time_series_raw)
            st.session_state.time_series_analysis = analysis_result
        
        st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ä—è–¥–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
        st.write(analysis_result)
    else:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª XLSX.")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É
if st.session_state.get('time_series_raw') and st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä—è–¥–æ–≤ –¥–∞–Ω–Ω—ã—Ö"):
    st.subheader("–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤")
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        markdown_data = convert_excel_to_markdown_for_analysis(st.session_state.time_series_raw)
        st.markdown(markdown_data)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")

#if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ", disabled=st.session_state.processing):
#    generate_response()

if st.session_state.report_content and not st.session_state.processing:
    st.divider()
    st.subheader("–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    b64_txt = base64.b64encode(st.session_state.report_content.encode()).decode()
    txt_href = f'<a href="data:file/txt;base64,{b64_txt}" download="report.txt">üì• –°–∫–∞—á–∞—Ç—å TXT Markdown –æ—Ç—á–µ—Ç</a>'
    st.markdown(txt_href, unsafe_allow_html=True)
    
    try:
        html_bytes = create_html_report(st.session_state.report_content, "–û—Ç—á–µ—Ç Troubleshooter")
        b64_html = base64.b64encode(html_bytes).decode()
        html_href = f'<a href="data:text/html;base64,{b64_html}" download="report.html">üì• –°–∫–∞—á–∞—Ç—å HTML –æ—Ç—á–µ—Ç</a>'
        st.markdown(html_href, unsafe_allow_html=True)
        
        with st.expander("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –æ—Ç—á–µ—Ç–∞"):
            st.components.v1.html(html_bytes.decode('utf-8'), height=600, scrolling=True)
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ HTML –æ—Ç—á–µ—Ç–∞: {str(e)}")

if st.session_state.processing:
    st.info("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
